{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reactionmodel.load\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import NamedTuple\n",
    "from reactionmodel.specification import SimulationSpecification\n",
    "\n",
    "from simulators import SIMULATORS\n",
    "\n",
    "@dataclass\n",
    "class SimulatorArguments():\n",
    "    t_span: tuple\n",
    "    sample_points: tuple\n",
    "\n",
    "TEST_ARGUMENTS = SimulatorArguments((0.0, 50.0), np.linspace(0, 50, 51))\n",
    "\n",
    "inital_cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(inital_cwd)\n",
    "test_dir = \"./tests/sbml-tests/\"\n",
    "tests = glob.glob(os.path.join(test_dir, 'sbml-*'))\n",
    "\n",
    "target_test = \"sbml-001-birth-death\"\n",
    "target_check = \"p01ic01\"\n",
    "target_catchment = []\n",
    "\n",
    "specifications = {}\n",
    "\n",
    "for test in tests:\n",
    "    if os.path.basename(test) == target_test:\n",
    "        run_tests_from_dir(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_simulations(s, n=1000, config_path=''):\n",
    "    results = []\n",
    "    simulator = s.simulator\n",
    "    forward_time = SIMULATORS[simulator]\n",
    "    rng = np.random.default_rng()\n",
    "    initial_condition = s.model.make_initial_condition(s.initial_condition)\n",
    "    simulation_options = s.simulation_options.copy()\n",
    "\n",
    "    if simulator == 'hybrid':\n",
    "        import hybrid\n",
    "        partition_path = simulation_options.pop('partition')\n",
    "        print(os.path.join(config_path, partition_path))\n",
    "        partition_scheme = hybrid.load_partition_scheme(os.path.join(config_path, partition_path))\n",
    "        simulation_options['partition_function'] = partition_scheme.partition_function\n",
    "    s.model.bake_k(parameters=s.parameters, jit=True)\n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "        result = forward_time(initial_condition, TEST_ARGUMENTS.t_span, s.model.k_jit, s.model.stoichiometry(), s.model.rate_involvement(), rng, discontinuities=TEST_ARGUMENTS.sample_points, **simulation_options)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def run_tests_with_checks(root, specifications):\n",
    "    tests_to_do = []\n",
    "    for check in glob.glob(os.path.join(root, 'checks', '*/')):\n",
    "        check_dir_root = check.split('/')[-2]\n",
    "        if target_check is not None:\n",
    "            if check_dir_root != target_check:\n",
    "                print(f\"Since we are targeting a single check, we are skipping {check_dir_root}\")\n",
    "                continue\n",
    "        tests_to_do.append((specifications[check_dir_root], check))\n",
    "    print(f\"Performing tests with benchmark data for {len(tests_to_do)}/{len(specifications)} combinations.\")\n",
    "\n",
    "    for spec_tuple,check_dir in tests_to_do:\n",
    "        specification, config_dir = spec_tuple\n",
    "        check_data = pd.read_csv(glob.glob(os.path.join(check_dir, '*.csv'))[0])\n",
    "        results = do_simulations(specification, config_path=config_dir)\n",
    "        desired_species = set([c.split('-')[0] for c in check_data.columns if len(c.split('-')) > 1])\n",
    "        assert len(desired_species) == 1\n",
    "        desired_species = list(desired_species)[0]\n",
    "        target_index = [s.name for s in specification.model.species].index(desired_species)\n",
    "        print(\"TARGET INDEX: \", target_index)\n",
    "        aligned = align_results(results, check_data['time'], target_index, desired_species)\n",
    "        if target_check:\n",
    "            print(\"Catching our targeted check in target_catchment\")\n",
    "            target_catchment.append((aligned, check_data))\n",
    "        return aligned\n",
    "\n",
    "def align_results(results, time, target_index, species_name):\n",
    "    all_aligned = []\n",
    "    for r in results:\n",
    "        aligned = []\n",
    "        t_history = r.t_history\n",
    "        for t in time[:-1]:\n",
    "            idx = np.argmin(np.abs(t-t_history))\n",
    "            aligned.append((r.t_history[idx], r.y_history[target_index,idx]))\n",
    "        aligned.append((r.t, r.y[target_index]))\n",
    "        all_aligned.append(pd.DataFrame.from_records(aligned, columns=['time', species_name]))\n",
    "    return all_aligned\n",
    "\n",
    "def get_files(root, individual, collection, pattern):\n",
    "    if os.path.isfile(os.path.join(root, individual)):\n",
    "        return [os.path.join(root, individual)]\n",
    "    return glob.glob(os.path.join(root, collection, pattern))\n",
    "\n",
    "class SpecTuple(NamedTuple):\n",
    "    specification: SimulationSpecification\n",
    "    config_path: str\n",
    "\n",
    "def run_tests_from_dir(dir):\n",
    "    model_paths  = get_files(dir, 'model.txt', 'models', 'model*.txt')\n",
    "    params_paths = get_files(dir, 'parameters.txt', 'parameters', 'parameters*.txt')\n",
    "    config_paths = get_files(dir, 'config.txt', 'configurations', 'config*.txt')\n",
    "    ic_paths     = get_files(dir, 'ic.txt', 'initial_conditions', 'initial*.txt')\n",
    "    for model_path in model_paths:\n",
    "        for params_path in params_paths:\n",
    "            for config_path in config_paths:\n",
    "                for ic_path in ic_paths:\n",
    "                    specification = reactionmodel.load.load_specification(model_path, params_path, config_path, ic_path)\n",
    "                    # use the parameter and ic file names as a unique identifier for this combination\n",
    "                    # later, we will look up all the combinations that we have test data for, and run simulations to check\n",
    "                    model_match = re.search('[a-z]+([0-9]+)\\.txt', model_path)\n",
    "                    config_match = re.search('[a-z]+([0-9]+)\\.txt', config_path)\n",
    "                    param_match = re.search('[a-z]+([0-9]+)\\.txt', params_path)\n",
    "                    ic_match = re.search('[a-z]+([0-9]+)\\.txt', ic_path)\n",
    "                    matches = [('m', model_match), ('c', config_match), ('p', param_match), ('ic', ic_match)]\n",
    "                    identifier = ''\n",
    "                    for id_str, match in matches:\n",
    "                        if match:\n",
    "                            identifier += id_str + str(match[1])\n",
    "                    # if identifier == '': all of the configuration files lived in root directory, so the check should just live in the root of the check directory\n",
    "                    specifications[identifier] = SpecTuple(specification, os.path.dirname(config_path))\n",
    "    run_tests_with_checks(dir, specifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_target = target_catchment[0]\n",
    "results, check = single_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_results = []\n",
    "for r in results:\n",
    "    r['time'] = np.round(r['time'], 5)\n",
    "    r = r.set_index('time')\n",
    "    indexed_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_check = pd.concat([pd.concat(indexed_results, axis=1).mean(axis=1), pd.concat(indexed_results, axis=1).std(axis=1)], axis=1)\n",
    "results_to_check.columns=['mean', 'std']\n",
    "results_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sbmlteam/sbml-test-suite/blob/release/cases/stochastic/DSMTS-userguide-31v2.pdf\n",
    "z_t = (results_to_check['mean'] - check['X-mean'])/(check['X-sd']) * np.sqrt(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
