{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reactionmodel.load\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import NamedTuple\n",
    "from reactionmodel.specification import SimulationSpecification\n",
    "\n",
    "from simulators import SIMULATORS\n",
    "import test\n",
    "\n",
    "@dataclass\n",
    "class SimulatorArguments():\n",
    "    t_span: tuple\n",
    "    t_eval: tuple\n",
    "\n",
    "TEST_ARGUMENTS = SimulatorArguments((0.0, 50.0), np.linspace(0, 50, 51))\n",
    "\n",
    "inital_cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_simulations(s, n=200):\n",
    "    results = []\n",
    "    simulator = s.simulator\n",
    "    forward_time = SIMULATORS[simulator]\n",
    "    rng = np.random.default_rng()\n",
    "    initial_condition = s.model.make_initial_condition(s.initial_condition)\n",
    "    simulation_options = s.simulation_options.copy()\n",
    "\n",
    "    if simulator == 'hybrid':\n",
    "        import hybrid\n",
    "        partition_path = simulation_options.pop('partition')\n",
    "        partition_scheme = hybrid.load_partition_scheme(partition_path)\n",
    "        simulation_options['partition_function'] = partition_scheme.partition_function\n",
    "    k = s.model.get_k(parameters=s.parameters, jit=True)\n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "        result = forward_time(initial_condition, TEST_ARGUMENTS.t_span, k, s.model.stoichiometry(), s.model.rate_involvement(), rng, discontinuities=TEST_ARGUMENTS.t_eval, **simulation_options)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def align_results(results, time, target_indices, species_names):\n",
    "    all_aligned = []\n",
    "    for r in results:\n",
    "        aligned = []\n",
    "        t_history = r.t_history\n",
    "        for t in time:\n",
    "            idx = np.argmin(np.abs(t-t_history))\n",
    "            aligned.append((r.t_history[idx], *[r.y_history[target_index,idx] for target_index in target_indices]))\n",
    "        all_aligned.append(pd.DataFrame.from_records(aligned, columns=['time', *species_names]))\n",
    "    \n",
    "    indexed_results = []\n",
    "    for r in all_aligned:\n",
    "        r['time'] = np.round(r['time'], 5)\n",
    "        r = r.set_index('time')\n",
    "        indexed_results.append(r)\n",
    "\n",
    "    return indexed_results\n",
    "\n",
    "def z_score_for_mean(aligned_results, check_data):\n",
    "    target_species = set([c.split('-')[0] for c in check_data.columns if len(c.split('-')) > 1])\n",
    "\n",
    "    df = pd.concat(aligned_results, axis=1)\n",
    "    results_to_check = pd.concat([df.groupby(by=df.columns, axis=1).mean(), df.groupby(by=df.columns, axis=1).std()], axis=1)\n",
    "    results_to_check.columns = [c + '-mean' if i < len(target_species) else c + '-sd' for i,c in enumerate(results_to_check.columns)]\n",
    "\n",
    "    # https://github.com/sbmlteam/sbml-test-suite/blob/release/cases/stochastic/DSMTS-userguide-31v2.pdf\n",
    "    z_ts = {}\n",
    "    for species in target_species:\n",
    "        z_t = (results_to_check[f'{species}-mean'] - check_data[f'{species}-mean'])/(check_data[f'{species}-sd']) * np.sqrt(n)\n",
    "        z_ts[species] = z_t\n",
    "\n",
    "    return results_to_check, z_ts\n",
    "\n",
    "class TestResult(NamedTuple):\n",
    "    results_df: pd.DataFrame\n",
    "    check_df: pd.DataFrame\n",
    "    z_scores_for_mean_by_species: dict\n",
    "\n",
    "def single_test(specification, check_path, **kwargs):\n",
    "    check_data = pd.read_csv(check_path)\n",
    "    results = do_simulations(specification, **kwargs)\n",
    "    desired_species = set([c.split('-')[0] for c in check_data.columns if len(c.split('-')) > 1])\n",
    "    all_species = [s.name for s in specification.model.species]\n",
    "    targets = [all_species.index(s) for s in desired_species]\n",
    "    aligned = align_results(results, check_data['time'], targets, desired_species)\n",
    "\n",
    "    results_table, z_ts = z_score_for_mean(aligned, check_data)\n",
    "    \n",
    "    return TestResult(results_table, check_data, z_ts)\n",
    "\n",
    "def run_tests_with_checks(root, specifications, **kwargs):\n",
    "    tests_to_do = []\n",
    "    for check in glob.glob(os.path.join(root, 'checks', '*/')):\n",
    "        check_dir_root = check.split('/')[-2]\n",
    "        tests_to_do.append(specifications[check_dir_root])\n",
    "    print(f\"Performing tests with benchmark data for {len(tests_to_do)}/{len(specifications)} combinations.\")\n",
    "\n",
    "    test_results = {}\n",
    "    for specification,check_dir in tests_to_do:\n",
    "        # each check directory contains 1 CSV file with a name like check{SBML_TEST_NUMBER}.csv\n",
    "        assert(len(glob.glob(os.path.join(check_dir, 'check*.csv')))) == 1, f\"Check directory {check_dir} had more than 1 check csv. I don't know what to do\"\n",
    "        check_file = glob.glob(os.path.join(check_dir, '*.csv'))[0]\n",
    "        test_results[check_dir] = single_test(specification, check_file, **kwargs)\n",
    "    return test_results\n",
    "\n",
    "def get_files(root, individual, collection, pattern):\n",
    "    if os.path.isfile(os.path.join(root, individual)):\n",
    "        return [os.path.join(root, individual)]\n",
    "    return glob.glob(os.path.join(root, collection, pattern))\n",
    "\n",
    "class SpecTuple(NamedTuple):\n",
    "    specification: SimulationSpecification\n",
    "    config_path: str\n",
    "\n",
    "def run_tests_from_dir(dir, **kwargs):\n",
    "    model_paths  = get_files(dir, 'model.txt', 'models', 'model*.txt')\n",
    "    params_paths = get_files(dir, 'parameters.txt', 'parameters', 'parameters*.txt')\n",
    "    config_paths = get_files(dir, 'config.txt', 'configurations', 'config*.txt')\n",
    "    ic_paths     = get_files(dir, 'ic.txt', 'initial_conditions', 'initial*.txt')\n",
    "    specifications = {}\n",
    "    for model_path in model_paths:\n",
    "        for params_path in params_paths:\n",
    "            for config_path in config_paths:\n",
    "                for ic_path in ic_paths:\n",
    "                    specification = reactionmodel.load.load_specification(model_path, params_path, config_path, ic_path)\n",
    "                    # use the parameter and ic file names as a unique identifier for this combination\n",
    "                    # later, we will look up all the combinations that we have test data for, and run simulations to check\n",
    "                    model_match = re.search('[a-z]+([0-9]+)\\.txt', model_path)\n",
    "                    config_match = re.search('[a-z]+([0-9]+)\\.txt', config_path)\n",
    "                    param_match = re.search('[a-z]+([0-9]+)\\.txt', params_path)\n",
    "                    ic_match = re.search('[a-z]+([0-9]+)\\.txt', ic_path)\n",
    "                    matches = [('m', model_match), ('c', config_match), ('p', param_match), ('i', ic_match)]\n",
    "                    identifier = ''\n",
    "                    for id_str, match in matches:\n",
    "                        if match:\n",
    "                            identifier += id_str + str(match[1])\n",
    "                    # if identifier == '': all of the configuration files lived in root directory, so the check should just live in the root of the check directory\n",
    "                    specifications[identifier] = SpecTuple(specification, os.path.dirname(config_path))\n",
    "    return run_tests_with_checks(dir, specifications, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(inital_cwd)\n",
    "test_dir = \"./tests/sbml-tests/\"\n",
    "tests = glob.glob(os.path.join(test_dir, 'sbml-*'))\n",
    "\n",
    "target_test = \"sbml-003-dimerisation\"\n",
    "#target_test = \"sbml-001-birth-death\"\n",
    "target_check = \"p01i01\"\n",
    "\n",
    "n = 10\n",
    "\n",
    "specification = reactionmodel.load.load_specification(*test.get_path_tuple(os.path.join(test_dir, target_test), target_check))\n",
    "check_file = glob.glob(os.path.join(test_dir, target_test, 'checks', target_check, '*.csv'))[0]\n",
    "test_result = single_test(specification, check_file, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species, series in test_result.z_scores_for_mean_by_species.items():\n",
    "    print(species)\n",
    "    print(series)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
